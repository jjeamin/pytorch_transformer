{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "\n",
    "Reference : [https://github.com/ndb796/Deep-Learning-Paper-Review-and-Practice/blob/master/code_practices/Attention_is_All_You_Need_Tutorial_(German_English).ipynb](https://github.com/ndb796/Deep-Learning-Paper-Review-and-Practice/blob/master/code_practices/Attention_is_All_You_Need_Tutorial_(German_English).ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "import spacy\n",
    "from torchtext.data import Field\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import BucketIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_multi30k(lower=True, batch_first=True, min_freq=2):\n",
    "    spacy_de = spacy.load('de')\n",
    "    spacy_en = spacy.load('en')\n",
    "\n",
    "\n",
    "    def tokenize_de(text):    \n",
    "        return [token.text for token in spacy_de.tokenizer(text)]\n",
    "\n",
    "\n",
    "    def tokenize_en(text):\n",
    "        return [token.text for token in spacy_en.tokenizer(text)]\n",
    "        \n",
    "\n",
    "    SRC = Field(tokenize=tokenize_de, init_token=\"<sos>\", eos_token=\"<eos>\", lower=lower, batch_first=batch_first)\n",
    "    TRG = Field(tokenize=tokenize_en, init_token=\"<sos>\", eos_token=\"<eos>\", lower=lower, batch_first=batch_first)\n",
    "\n",
    "    train_dataset, valid_dataset, test_dataset = Multi30k.splits(exts=(\".de\", \".en\"), fields=(SRC, TRG))\n",
    "\n",
    "    SRC.build_vocab(train_dataset, min_freq=min_freq)\n",
    "    TRG.build_vocab(train_dataset, min_freq=min_freq)\n",
    "\n",
    "    return train_dataset, valid_dataset, test_dataset, SRC, TRG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout, device='cuda'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model // n_heads \n",
    "\n",
    "        self.wq = nn.Linear(d_model, d_model)\n",
    "        self.wk = nn.Linear(d_model, d_model)\n",
    "        self.wv = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.wo = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.attention = None\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "\n",
    "    def forward(self, query, key, value, mask = None):\n",
    "\n",
    "        batch_size = query.shape[0]\n",
    "\n",
    "        Q = self.wq(query)\n",
    "        K = self.wk(key)\n",
    "        V = self.wv(value)\n",
    "\n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "\n",
    "        # [batch_size, n_heads, query_len, head_dim]\n",
    "        # [batch_size, n_heads, key_len  , head_dim]\n",
    "        # [batch_size, n_heads, value_len, head_dim]\n",
    "\n",
    "        # Self Attention\n",
    "        # Softmax(dot(Q, K.T) / scale) * V\n",
    "        ##############################################################\n",
    "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
    "\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask==0, -1e10)\n",
    "            \n",
    "        self.attention = torch.softmax(energy, dim=-1)\n",
    "        x = torch.matmul(self.dropout(self.attention), V)\n",
    "        ##############################################################\n",
    "        # x: [batch_size, n_heads, query_len, head_dim]\n",
    "\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "\n",
    "        # x: [batch_size, query_len, n_heads, head_dim]\n",
    "\n",
    "        x = x.view(batch_size, -1, self.d_model)\n",
    "\n",
    "        # x: [batch_size, query_len, hidden_dim]\n",
    "\n",
    "        x = self.wo(x)\n",
    "\n",
    "        # x: [batch_size, query_len, hidden_dim]\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.w1 = nn.Linear(d_model, d_ff)\n",
    "        self.w2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.w1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.w2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attention_norm = nn.LayerNorm(d_model)\n",
    "        self.self_attention = MultiHeadAttentionLayer(d_model, n_heads, dropout, device)\n",
    "        \n",
    "        self.feedforward = FeedForward(d_model, d_ff, dropout)\n",
    "        self.feedforward_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # x    : [batch_size, src_len, d_model]\n",
    "        # mask : [batch_size, src_len]\n",
    "\n",
    "        # 필요한 경우 마스크(mask) 행렬을 이용하여 어텐션(attention)할 단어를 조절 가능\n",
    "        y = self.self_attention(x, x, x, mask)\n",
    "        x = self.self_attention_norm(x + self.dropout(y))\n",
    "\n",
    "        # [batch_size, src_len, d_model]\n",
    "        y = self.feedforward(x)\n",
    "        x = self.feedforward_norm(x + self.dropout(y))\n",
    "\n",
    "        # [batch_size, src_len, d_model]\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, source_size, d_model, N, n_heads, d_ff, dropout, device, max_length=100):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.tok_embedding = nn.Embedding(source_size, d_model)\n",
    "        self.pos_embedding = nn.Embedding(max_length, d_model)\n",
    "\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model, n_heads, d_ff, dropout, device) for _ in range(N)])\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([d_model])).to(device)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # x    : [batch_size, src_len]\n",
    "        # mask : [batch_size, src_len]\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "        x_len = x.shape[1]\n",
    "        \n",
    "        pos = torch.arange(0, x_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        # x([batch_size, x_len, d_model]) + pos([batch_size, x_len, d_model])\n",
    "        x = self.dropout((self.tok_embedding(x) * self.scale) + self.pos_embedding(pos))\n",
    "\n",
    "        # x: [batch_size, src_len, d_model]\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "\n",
    "        # x: [batch_size, src_len, d_model]\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attention = MultiHeadAttentionLayer(d_model, n_heads, dropout, device)\n",
    "        self.self_attention_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.encoder_attention = MultiHeadAttentionLayer(d_model, n_heads, dropout, device)\n",
    "        self.encoder_attention_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.feedforward = FeedForward(d_model, d_ff, dropout)\n",
    "        self.feedforward_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    # 인코더의 출력 값(enc_src)을 어텐션(attention)하는 구조\n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "\n",
    "        # trg     : [batch_size, trg_len, hidden_dim]\n",
    "        # enc_src : [batch_size, src_len, hidden_dim]\n",
    "        # trg_mask: [batch_size, trg_len]\n",
    "        # src_mask: [batch_size, src_len]\n",
    "\n",
    "        # self attention\n",
    "        # 자기 자신에 대하여 어텐션(attention)\n",
    "        _trg = self.self_attention(trg, trg, trg, trg_mask)\n",
    "        trg = self.self_attention_norm(trg + self.dropout(_trg))\n",
    "\n",
    "        # trg: [batch_size, trg_len, hidden_dim]\n",
    "\n",
    "        # encoder attention\n",
    "        # 디코더의 쿼리(Query)를 이용해 인코더를 어텐션(attention)\n",
    "        _trg = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
    "        trg = self.encoder_attention_norm(trg + self.dropout(_trg))\n",
    "\n",
    "        # trg: [batch_size, trg_len, hidden_dim]\n",
    "\n",
    "        _trg = self.feedforward(trg)\n",
    "        trg = self.feedforward_norm(trg + self.dropout(_trg))\n",
    "\n",
    "        # trg: [batch_size, trg_len, hidden_dim]\n",
    "\n",
    "        return trg\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, target_size, d_model, N, n_heads, d_ff, dropout, device, max_length=100):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.tok_embedding = nn.Embedding(target_size, d_model)\n",
    "        self.pos_embedding = nn.Embedding(max_length, d_model)\n",
    "\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, n_heads, d_ff, dropout, device) for _ in range(N)])\n",
    "\n",
    "        self.fc_out = nn.Linear(d_model, target_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([d_model])).to(device)\n",
    "\n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "\n",
    "        # trg     : [batch_size, trg_len]\n",
    "        # enc_src : [batch_size, src_len, d_model]\n",
    "        # trg_mask: [batch_size, trg_len]\n",
    "        # src_mask: [batch_size, src_len]\n",
    "\n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "\n",
    "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "\n",
    "        # pos: [batch_size, trg_len]\n",
    "\n",
    "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
    "\n",
    "        # trg: [batch_size, trg_len, hidden_dim]\n",
    "\n",
    "        for layer in self.layers:\n",
    "            trg = layer(trg, enc_src, trg_mask, src_mask)\n",
    "\n",
    "        # trg: [batch_size, trg_len, hidden_dim]\n",
    "\n",
    "        output = self.fc_out(trg)\n",
    "\n",
    "        # output: [batch_size, trg_len, output_dim]\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, source_size, target_size, d_model, enc_N, dec_N, n_heads, d_ff, dropout, device):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(source_size, d_model, enc_N, n_heads, d_ff, dropout, device)\n",
    "        self.decoder = Decoder(target_size, d_model, dec_N, n_heads, d_ff, dropout, device)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "\n",
    "        # src: [batch_size, src_len]\n",
    "        # trg: [batch_size, trg_len]\n",
    "\n",
    "        src_mask = make_src_mask(src, 1)\n",
    "        trg_mask = make_trg_mask(trg, 1, self.device)\n",
    "\n",
    "        # src_mask: [batch_size, 1, 1, src_len]\n",
    "        # trg_mask: [batch_size, 1, trg_len, trg_len]\n",
    "\n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "\n",
    "        # enc_src: [batch_size, src_len, d_model]\n",
    "\n",
    "        output = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
    "\n",
    "        # output: [batch_size, trg_len, target_size]\n",
    "        # attention: [batch_size, n_heads, trg_len, src_len]\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_src_mask(src, src_pad_idx):\n",
    "    src_mask = (src != src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "    \n",
    "    return src_mask\n",
    "\n",
    "\n",
    "# 타겟 문장에서 각 단어는 다음 단어가 무엇인지 알 수 없도록(이전 단어만 보도록) 만들기 위해 마스크를 사용\n",
    "def make_trg_mask(trg, trg_pad_idx, device):\n",
    "\n",
    "    # trg: [batch_size, trg_len]\n",
    "\n",
    "    trg_pad_mask = (trg != trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "    \n",
    "    # trg_pad_mask: [batch_size, 1, 1, trg_len]\n",
    "\n",
    "    trg_len = trg.shape[1]\n",
    "\n",
    "    trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = device)).bool()\n",
    "\n",
    "    # trg_sub_mask: [trg_len, trg_len]\n",
    "\n",
    "    trg_mask = trg_pad_mask & trg_sub_mask\n",
    "\n",
    "    # trg_mask: [batch_size, 1, trg_len, trg_len]\n",
    "\n",
    "    return trg_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[True, True, True, True, True]]],\n",
       "\n",
       "\n",
       "        [[[True, True, True, True, True]]]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_src_mask(torch.Tensor([[2,3,4,5,6], [2,3,4,5,6]]), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ True, False, False, False, False],\n",
       "          [ True, False, False, False, False],\n",
       "          [ True, False,  True, False, False],\n",
       "          [ True, False,  True,  True, False],\n",
       "          [ True, False,  True,  True,  True]]],\n",
       "\n",
       "\n",
       "        [[[ True, False, False, False, False],\n",
       "          [ True,  True, False, False, False],\n",
       "          [ True,  True,  True, False, False],\n",
       "          [ True,  True,  True,  True, False],\n",
       "          [ True,  True,  True,  True,  True]]]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_trg_mask(torch.Tensor([[2,1,4,5,6], [2,3,4,5,6]]), 1, 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for i, batch in enumerate(iterator):\n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # source : <sos>     ~   <eos>\n",
    "        # target : <sos>     ~   <eos> -1\n",
    "        # output : <sos> + 1 ~   <eos>\n",
    "        \n",
    "        output = model(src, trg[:,:-1])\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        trg = trg[:,1:].contiguous().view(-1)\n",
    "\n",
    "        # output : [batch size * trg_len - 1, output_dim]\n",
    "        # target : [batch size * trg len - 1]\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "\n",
    "        # 기울기(gradient) clipping 진행\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            # source : <sos>     ~   <eos>\n",
    "            # target : <sos>     ~   <eos> -1\n",
    "            # output : <sos> + 1 ~   <eos>\n",
    "            \n",
    "            output = model(src, trg[:,:-1])\n",
    "\n",
    "            # output: [배치 크기, trg_len - 1, output_dim]\n",
    "            # trg: [배치 크기, trg_len]\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "\n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            # 출력 단어의 인덱스 0(<sos>)은 제외\n",
    "            trg = trg[:,1:].contiguous().view(-1)\n",
    "\n",
    "            # output : [batch size * trg_len - 1, output_dim]\n",
    "            # target : [batch size * trg len - 1]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, src_field, trg_field, model, device, max_len=50, logging=True):\n",
    "    model.eval()\n",
    "\n",
    "    if isinstance(sentence, str):\n",
    "        nlp = spacy.load('de')\n",
    "        tokens = [token.text.lower() for token in nlp(sentence)]\n",
    "    else:\n",
    "        tokens = [token.lower() for token in sentence]\n",
    "\n",
    "    # tokens : <sos> ~ <eos>\n",
    "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
    "    if logging:\n",
    "        print(f\"전체 소스 토큰: {tokens}\")\n",
    "\n",
    "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
    "    if logging:\n",
    "        print(f\"소스 문장 인덱스: {src_indexes}\")\n",
    "\n",
    "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
    "    src_mask = make_src_mask(src_tensor, 1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        enc_src = model.encoder(src_tensor, src_mask)\n",
    "\n",
    "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
    "\n",
    "    for i in range(max_len):\n",
    "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
    "        trg_mask = make_trg_mask(trg_tensor, 1, device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
    "\n",
    "        # 출력 문장에서 가장 마지막 단어만 사용\n",
    "        pred_token = output.argmax(2)[:,-1].item()\n",
    "        trg_indexes.append(pred_token)\n",
    "\n",
    "        # endpoint : get <eos>\n",
    "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
    "            break\n",
    "\n",
    "    # index to word\n",
    "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
    "\n",
    "    return trg_tokens[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "# Get Data\n",
    "train_dataset, valid_dataset, test_dataset, SRC, TRG = get_multi30k()\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits((train_dataset, valid_dataset, test_dataset),\n",
    "                                                                       batch_size=128,\n",
    "                                                                       device=device)\n",
    "                                                                       \n",
    "source_size = len(SRC.vocab)\n",
    "target_size = len(TRG.vocab)\n",
    "d_model = 256\n",
    "enc_N = 3\n",
    "dec_N = 3\n",
    "n_heads = 8\n",
    "d_ff = 512\n",
    "dropout = 0.1\n",
    "\n",
    "# Seq2Seq\n",
    "SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n",
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer 객체 선언\n",
    "model = Transformer(source_size, \n",
    "                    target_size, \n",
    "                    d_model, \n",
    "                    enc_N, \n",
    "                    dec_N, \n",
    "                    n_heads, \n",
    "                    d_ff, \n",
    "                    dropout,\n",
    "                    device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip = 1\n",
    "epochs = 10\n",
    "lr = 0.0005\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Time: 0m 16s\n",
      "\tTrain Loss: 8.837 | Train PPL: 6885.658\n",
      "\tValidation Loss: 8.835 | Validation PPL: 6868.007\n",
      "소스 문장: ['ein', 'mann', 'mit', 'einem', 'orangefarbenen', 'hut', ',', 'der', 'etwas', 'anstarrt', '.']\n",
      "타겟 문장: ['a', 'man', 'in', 'an', 'orange', 'hat', 'starring', 'at', 'something', '.']\n",
      "전체 소스 토큰: ['<sos>', 'ein', 'mann', 'mit', 'einem', 'orangefarbenen', 'hut', ',', 'der', 'etwas', 'anstarrt', '.', '<eos>']\n",
      "소스 문장 인덱스: [2, 5, 13, 11, 6, 175, 106, 9, 15, 75, 0, 4, 3]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "make_src_mask() missing 1 required positional argument: 'src_pad_idx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-95-514cc56a3944>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'타겟 문장: {trg}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0mtranslation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtranslate_sentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSRC\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTRG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"모델 출력 결과:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtranslation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-70-45fcd87b478e>\u001b[0m in \u001b[0;36mtranslate_sentence\u001b[1;34m(sentence, src_field, trg_field, model, device, max_len, logging)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;31m# 소스 문장에 따른 마스크 생성\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0msrc_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_src_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;31m# 인코더(endocer)에 소스 문장을 넣어 출력 값 구하기\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: make_src_mask() missing 1 required positional argument: 'src_pad_idx'"
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time() # 시작 시간 기록\n",
    "\n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, clip)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "    end_time = time.time() # 종료 시간 기록\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'transformer_german_to_english.pt')\n",
    "\n",
    "    print(f'Epoch: {epoch + 1} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):.3f}')\n",
    "    print(f'\\tValidation Loss: {valid_loss:.3f} | Validation PPL: {math.exp(valid_loss):.3f}')\n",
    "\n",
    "    src = vars(test_dataset.examples[0])['src']\n",
    "    trg = vars(test_dataset.examples[0])['trg']\n",
    "\n",
    "    print(f'소스 문장: {src}')\n",
    "    print(f'타겟 문장: {trg}')\n",
    "\n",
    "    translation = translate_sentence(src, SRC, TRG, model, device, logging=True)\n",
    "\n",
    "    print(\"모델 출력 결과:\", \" \".join(translation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
